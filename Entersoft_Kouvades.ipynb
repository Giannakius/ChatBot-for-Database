{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TLiyL0mk4vOg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "944a8f1a-5870-4f64-b31a-7d6288edaeac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m817.7/817.7 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m525.5/525.5 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.3/291.3 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.2/115.2 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.9/91.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m92.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        " pip install langchain_openai langchain_community langchain pymysql chromadb pyngrok flask -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qtdqWJnMOMir"
      },
      "outputs": [],
      "source": [
        " import os\n",
        " os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-J6yFWZCa46t4cURpxo6uT3BlbkFJ912Jhz3bwreeCyDQXEOi\"\n",
        "\n",
        " db_user = \"hackathon\"\n",
        " db_password = \"hackathon\"\n",
        " db_host = \"panosgio.org\"\n",
        " db_name = \"hackathon\"\n",
        " from langchain_community.utilities.sql_database import SQLDatabase\n",
        " # db = SQLDatabase.from_uri(f\"mysql+pymysql://{db_user}:{db_password}@{db_host}/{db_name}\",sample_rows_in_table_info=1,include_tables=['customers','orders'],custom_table_info={'customers':\"customer\"})\n",
        " db = SQLDatabase.from_uri(f\"mysql+pymysql://{db_user}:{db_password}@{db_host}/{db_name}\")\n",
        " #print(db.dialect)\n",
        " #print(db.get_usable_table_names())\n",
        " #print(db.table_info)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JWeywzkROjuh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a96eb6a-ef9c-47b0-abde-1198df188217"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SELECT `Product Description`, `Revenue` \n",
            "FROM `LastWeekTopProducts` \n",
            "ORDER BY `Revenue` DESC \n",
            "LIMIT 6;\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import create_sql_query_chain\n",
        "from langchain_openai import ChatOpenAI\n",
        "# question = 'Find how much revenue did we make from the top 10 customers and then sum the value of the 10 first only'\n",
        "question = 'Can you list please the top six products?'\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "generate_query = create_sql_query_chain(llm, db)\n",
        "query = generate_query.invoke({\"question\": question})\n",
        "# \"what is price of `1968 Ford Mustang`\"\n",
        "print(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jb-vuNcoP9Y8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "5a9607ed-949e-4cee-b12c-d09c1dce24a4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"[('Selmer Paris Reference 54 Alto', 409500), ('Keilwerth SX90R', 264000), ('Roland Fantom-8', 248500), ('Tama Starclassic Bubinga 5-Piece Shell Pack', 245000), ('Music Man John Petrucci Majesty', 240000), ('Nord Stage 3 Compact', 227500)]\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool\n",
        "execute_query = QuerySQLDataBaseTool(db=db)\n",
        "execute_query.invoke(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5XOLwmt_RcCQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "25f4d4d1-865d-4197-8d0b-e9a76389a78f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The top six products based on revenue are:\\n1. Selmer Paris Reference 54 Alto - $409,500\\n2. Keilwerth SX90R - $264,000\\n3. Roland Fantom-8 - $248,500\\n4. Tama Starclassic Bubinga 5-Piece Shell Pack - $245,000\\n5. Music Man John Petrucci Majesty - $240,000\\n6. Nord Stage 3 Compact - $227,500'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        " from operator import itemgetter\n",
        "\n",
        " from langchain_core.output_parsers import StrOutputParser\n",
        " from langchain_core.prompts import PromptTemplate\n",
        " from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        " answer_prompt = PromptTemplate.from_template(\n",
        "     \"\"\"Given the following user question, corresponding SQL query, and SQL result, answer the user question.\n",
        "\n",
        " Question: {question}\n",
        " SQL Query: {query}\n",
        " SQL Result: {result}\n",
        " Answer: \"\"\"\n",
        " )\n",
        "\n",
        " rephrase_answer = answer_prompt | llm | StrOutputParser()\n",
        "\n",
        " chain = (\n",
        "     RunnablePassthrough.assign(query=generate_query).assign(\n",
        "         result=itemgetter(\"query\") | execute_query\n",
        "     )\n",
        "     | rephrase_answer\n",
        " )\n",
        "\n",
        " chain.invoke({\"question\": question})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RiSa5kn44S1",
        "outputId": "cd273040-f2e4-467b-fd71-2b27b141bb10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Tunnel URL: https://59fa-35-233-196-116.ngrok-free.app\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Apr/2024 09:25:39] \"GET / HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Apr/2024 09:25:40] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Apr/2024 09:25:57] \"POST /run_query HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Apr/2024 09:32:35] \"POST /run_query HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Apr/2024 09:32:50] \"POST /run_query HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Apr/2024 09:33:44] \"POST /run_query HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Apr/2024 09:33:50] \"POST /run_query HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Apr/2024 09:34:03] \"POST /run_query HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Apr/2024 09:34:17] \"POST /run_query HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [21/Apr/2024 09:34:31] \"POST /run_query HTTP/1.1\" 200 -\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from flask import Flask, request, jsonify, render_template, send_from_directory, make_response\n",
        "from pyngrok import ngrok, conf\n",
        "import requests\n",
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool\n",
        "\n",
        "# Initialize Flask app\n",
        "\n",
        "conf.get_default().auth_token = \"2fNSqOSH8CLG8ChWeVy9kOmRxyA_3K2XfGTAfMQ8btmZ869vZ\"\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/')\n",
        "def index():\n",
        "    # Load HTML content\n",
        "    html_url = 'https://panosgio.org/hackathon/index.html'\n",
        "    html_response = requests.get(html_url)\n",
        "    html_content = html_response.text\n",
        "\n",
        "    # Load CSS content\n",
        "    css_url = 'https://panosgio.org/hackathon/ui.css'\n",
        "    css_response = requests.get(css_url)\n",
        "    css_content = css_response.text\n",
        "\n",
        "    # Load JavaScript content\n",
        "    js_url = 'https://panosgio.org/hackathon/ui.js'\n",
        "    js_response = requests.get(js_url)\n",
        "    js_content = js_response.text\n",
        "\n",
        "    # Embed CSS and JS into HTML\n",
        "    html_content = html_content.replace('<link rel=\"stylesheet\" href=\"ui.css\">', '<style>' + css_content + '</style>')\n",
        "    html_content = html_content.replace('<script src=\"ui.js\"></script>', '<script>' + js_content + '</script>')\n",
        "\n",
        "    # Return the modified HTML content\n",
        "    return make_response(html_content)\n",
        "\n",
        "# Define the endpoint\n",
        "@app.route('/run_query', methods=['POST'])\n",
        "def run_query():\n",
        "    # Extract question from the request\n",
        "    question = request.json.get('question')\n",
        "\n",
        "    # Your existing code to process the question\n",
        "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)  # Placeholder for language model initialization\n",
        "    generate_query = create_sql_query_chain(llm, db, k=1000)  # Assume this function exists\n",
        "    query = generate_query.invoke({\"question\": question})\n",
        "\n",
        "    # Use PromptTemplate to rephrase the answer\n",
        "    answer_prompt = PromptTemplate.from_template(\"write it to sql format. : SQL Query: {query} Answer: \")\n",
        "    rephrase_answer = answer_prompt | llm | StrOutputParser()\n",
        "\n",
        "    # Use RunnablePassthrough to manage data flow\n",
        "    execute_query = QuerySQLDataBaseTool(db=db)  # Placeholder for SQL execution\n",
        "    chain = (\n",
        "        RunnablePassthrough.assign(query=generate_query).assign(\n",
        "            result=itemgetter(\"query\") | execute_query\n",
        "        )\n",
        "        | rephrase_answer\n",
        "    )\n",
        "\n",
        "    query2 = chain.invoke({\"question\": question})\n",
        "\n",
        "    execute_query = QuerySQLDataBaseTool(db=db)\n",
        "    execute_query.invoke(query2)\n",
        "\n",
        "\n",
        "    answer_prompt = PromptTemplate.from_template(\n",
        "        \"\"\"Given the following user question, corresponding SQL query, and SQL result, answer the user question.\n",
        "\n",
        "    Question: {question}\n",
        "    SQL Query: {query}\n",
        "    SQL Result: {result}\n",
        "    Answer: \"\"\"\n",
        "    )\n",
        "\n",
        "    rephrase_answer = answer_prompt | llm | StrOutputParser()\n",
        "\n",
        "    chain = (\n",
        "        RunnablePassthrough.assign(query=generate_query).assign(\n",
        "            result=itemgetter(\"query\") | execute_query\n",
        "        )\n",
        "        | rephrase_answer\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Return the response\n",
        "    return jsonify({\"answer\": chain.invoke({\"question\": question})})\n",
        "\n",
        "# Run the Flask app on Colab\n",
        "if __name__ == '__main__':\n",
        "    public_url = ngrok.connect(5000).public_url\n",
        "    print(' * Tunnel URL:', public_url)\n",
        "    app.run(port=5000)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}